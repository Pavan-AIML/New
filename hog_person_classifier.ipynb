{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Exercise 01\n", "## Histogram of Orientated Gradients\n", "\n", "Even though nowadays there exist much better working object detectors than Histogram of Oriented Gradients, the historic approach was one of the first attempts to build feature descriptors, which encode the image with lower spatial and higher feature dimension in order to solve the matching problem."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Imports\n", "\n", "This will setup your whole environment such that you can work with the rest of the notebook.\n", "\n", "### General Imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "import torch\n", "import torch.optim as optim\n", "import torchvision.transforms as transforms\n", "from torch.utils.data import DataLoader"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Set up directory paths and (optionally) mount in Google Colab\n", "If you work with google colab set the `USING_COLAB` variable to `True` and following cell to mount your gdrive."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["USING_COLAB = False\n", "USE_CPU = True\n", "# Use the following lines if you want to use Google Colab\n", "# We presume you created a folder \"cv3dst\" within your main drive folder, and put the exercise there.\n", "# NOTE: terminate all other colab sessions that use GPU!\n", "# NOTE 2: Make sure the correct exercise folder (e.g exercise_03) is given.\n", "\n", "if USING_COLAB:\n", "    from google.colab import drive\n", "    import os\n", "\n", "    gdrive_path='/content/gdrive/MyDrive/cv3dst/exercise_01'\n", "\n", "    # This will mount your google drive under 'MyDrive'\n", "    drive.mount('/content/gdrive', force_remount=True)\n", "    # In order to access the files in this notebook we have to navigate to the correct folder\n", "    os.chdir(gdrive_path)\n", "    # Check manually if all files are present\n", "    print(sorted(os.listdir()))\n", "    root_dir = Path(gdrive_path).parent\n", "else:\n", "    # root_dir = Path('./cv3dst/')\n", "    root_dir = Path('/storage/remote/adm9/cv3dst/cv3ws23/cv3_exercises/cv3dst')\n", "dataset_dir = root_dir.joinpath(\"datasets\")\n", "output_dir = root_dir.joinpath('exercise_01', 'models')\n", "output_dir.mkdir(parents=True, exist_ok=True)\n", "\n", "device = torch.device('cuda') if torch.cuda.is_available() and not USE_CPU else torch.device('cpu')"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Exercise Specific Imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from exercise_code.test import (\n", "    test_compute_iou,\n", "    test_fill_hog_bins,\n", "    test_compute_image_gradient,\n", "    test_sliding_window_detection,\n", "    test_non_maximum_suppression,\n", ")\n", "from exercise_code import (\n", "    compute_image_gradient,\n", "    fill_hog_bins,\n", "    non_maximum_suppression,\n", "    MOT16HoG,\n", "    obj_detect_transforms,\n", "    train,\n", "    evaluate,\n", "    Net,\n", "    visualization,\n", "    sliding_window_detection,\n", "    extract_bbox_from_heatmap,\n", "    blockify_tensor,\n", ")\n", "\n", "%load_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part I - Loading the dataset\n", "\n", "In this exercise we will be working with the [MOT16](https://arxiv.org/abs/1603.00831) [1] dataset for multi-object tracking. Originally designed for people tracking - something you will deal with in the upcoming exercises -, we will use this dataset in this exercise for people detection. The dataset provides video sequences of images with multiple people. It also provides the bounding box annotations for people that we use as positiv examples in our training with the HOG features. However, the dataset does not provide negative examples that we need for our training scheme. \n", "\n", "In this exercise, we will create our own negative examples selecting random patches in the image. Naively sampling the image will not work, as there is a chance we sample an image patch with a person inside. In order to avoid this, we need to calculate the overlap of our random patch with all the positive examples. We do this by calculating the `Intersection over Union (IoU)` or `Jaccard Index` of two bounding boxes $A$ and $B$ defined as\n", "\n", "$$ J(A, B) = \\frac{A \\cap B}{A \\cup B}$$"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "    <h3>Task: IoU</h3>\n", "    <p>\n", "    Go to <code>exercise_code/model/compute_iou.py</code> and complete the <code>compute_iou</code> function. Be aware of the input shapes of the IoU function. We expect the function to compute the IoU between two batches of bounding boxes. The instruction in the code explain the input and output shapes in more detail.\n", "    </p>\n", "</div>"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    <h3>Test: IoU</h3>\n", "    <p> Run the following cell to execute the test case for the IoU. As always the test are done with small dummy examples and do <b>not</b> guarantee correct implementation.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_ = test_compute_iou()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["\n", "<p> For training our object detector, we need a dataset containing positive and negative samples (object/no object). The MOT dataset containes bounding box annotations around people, which serve as positive samples. We extended this dataset by adding negative samples for each image, filling the number of bounding boxes until <code>num_patches=50</code> for each image. The negatively labeled bounding boxes are randomly chosen with the constrain that its overlap with any positive bounding box annotation is smaller than <code>neg_th</code>.\n", "\n", "</p>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_train = MOT16HoG(dataset_dir.joinpath(\"MOT16\", \"train\"), transforms=obj_detect_transforms(train=False))\n", "dataset_test = MOT16HoG(dataset_dir.joinpath(\"MOT16\", \"test\"), random_offset=False, transforms=obj_detect_transforms(train=False))\n", ""]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["If loaded correctly, the length of the datasets should be:\n", "\n", "`3729 1587`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(len(dataset_train))\n", "print(len(dataset_test))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Visualizing examples of the dataset"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["We visualize one sample of the dataset by displaying all 50 bounding box/patch annotations. The red frame indicates that the patch is labeled as negative, whereas the green frame is used for positive annotations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img, target = dataset_test[0]\n", "\n", "rows, cols = 5, 10\n", "\n", "visualization.dataset(target[\"patches\"], target[\"patch_labels\"], rows, cols)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part II - Oriented Gradients\n", "\n", "Next up, we need to calculate the gradients of the image. We want to know both the magnitude as well as the orientation of the gradients at each pixel location. For this exercise, we use the central difference algorithm to calculate the gradient. You can choose other methods as well, but these are not checked by the test cases. With central differences the gradients at pixel location (x, y) are given by\n", "\n", "$$\\nabla_x I(x, y) = I(x + 1, y) - I(x - 1, y)\\,\\text{and}$$\n", "$$\\nabla_y I(x, y) = I(x, y + 1) - I(x, y - 1)\\,.$$\n", "\n", "Be aware of the coordinate conventions of the image and the order of rows/columns of pytorch tensors.\n", "\n", "From these gradients, we can calculate the norm/magnitude and orientation/angle of the gradients with\n", "$$\\nabla_n I(x, y) = \\left\\| \\begin{pmatrix} \\nabla_x I(x, y) \\\\ \\nabla_y I(x, y) \\end{pmatrix} \\right\\|_2\\,\\text{and}$$\n", "$$\\nabla_a I(x, y) = \\arctan \\frac{\\nabla_x I(x, y)}{\\nabla_y I(x, y)}\\,.$$\n", "\n", "Of course, the angle is only defined if $$\\nabla_y I(x, y) \\neq 0$$. Usually there are standard implementations that are able to deal with all these cases. See, if you can find them in pytorch."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "    <h3>Task: Orientated Gradients</h3>\n", "    <p> Go to <code>exercise_code/model/compute_image_gradients.py</code> and implement <code>compute_image_gradient</code> for a batch of images. The function returns the norm and orientation of the gradient for each pixel. The gradients will be needed to later compute a histogram for a certain image patch. </p> \n", "</div>"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    <h3>Test: Orientated Gradients</h3>\n", "    <p> Run the following cell to execute the test case for the Orientated Gradients. <b>Note:</b> This test checks the implementations of the gradients using <b>central difference</b>. As always the test are done with small dummy examples and do <b>not</b> guarantee correct implementation.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_ = test_compute_image_gradient()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Visualizing the Gradients\n", "Additionally to the test cases, you can also check your implementations of the gradient calculation by running the following code cells. They visualize the gradient norm and the gradient orientation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idx = 0\n", "patch_idx = 0\n", "\n", "img = dataset_train[idx][1][\"patches\"][patch_idx]\n", "\n", "gradient_norm, gradient_orientation = compute_image_gradient(transforms.functional.rgb_to_grayscale(img))\n", "visualization.gradients(img, gradient_norm.squeeze(-3), gradient_orientation.squeeze(-3))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["For the first example in the dataset (`idx=0,patch_idx=0`), the visualization should look something like this:\n", "\n", "![Example Visualization](./gradient_visualization.png)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part III - Histogram of Orientated Gradients\n", "\n", "Building a histogram over oriented gradients was one of the first attempts to decrease the spatial dimension while increasing the feature dimension. The first methods defined a certain cell size, containing $N \\times M$ (usually $8 \\times 8$) pixels. For each of these cells in an image, they build a histogram of the preciously computed gradient orientation and norm. \n", "\n", "The historgram consits of $K$ bins that quantize the angle values of the gradients into equally spaced regions of size $180 / K$ degrees. Now each pixel contributes to this histogram based on its orientation and magnitude. First the two closest bins need to be identified based on the starting angle value they represent. E.g. if $K=9$, then a gradient with an angle of $15$ degrees would contribute to bins $0$ $[0\\ldots20)$ degrees and $1$ $[20\\ldots40)$ degrees as it is between $0$ and $20$ degrees. \n", "\n", "$$ \\nabla_n I(x,y) \\frac{\\text{width} - d}{\\text{width}}$$\n", "\n", "is added to the bins based on the distance $d$ to the bin starting value."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "    <h3>Task: Histogram of Orientated Gradients</h3>\n", "    <p> \n", "    Your task is to build a histogram for each cell of the input image. Fot that go to <code>exercise_code/model/fill_hog_bins.py</code> and implement <code>fill_hog_bins</code> for a batch of cells.\n", "    </p>\n", "</div>"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    <h3>Test: Histogram of Orientated Gradients</h3>\n", "    <p> Run the following cell to execute the test case for the Histogram of Orientated Gradients. As always the test are done with small dummy examples and do <b>not</b> guarantee correct implementation.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_ = test_fill_hog_bins()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Visualizing HoG\n", "Additionally to the test cases, you can also check your implementations of the gradient calculation by running the following code cells. They visualize the gradient norm and the gradient orientation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idx = 0\n", "patch_idx = 0\n", "block_size = 8\n", "num_bins = 9\n", "\n", "img = dataset_train[idx][1][\"patches\"][patch_idx]\n", "\n", "gradient_norm, gradient_orientation = compute_image_gradient(transforms.functional.rgb_to_grayscale(img))\n", "gradient_orientation[gradient_orientation >= 180.0] = gradient_orientation[gradient_orientation >= 180.0] - 180.0\n", "hog_bins = fill_hog_bins(\n", "    blockify_tensor(gradient_norm.squeeze(-3), block_size).flatten(0, -3).flatten(-2, -1),\n", "    blockify_tensor(gradient_orientation.squeeze(-3), block_size).flatten(0, -3).flatten(-2, -1),\n", "    num_bins,\n", ")\n", "hog_bins = torch.roll(hog_bins,num_bins//2, dims=-1)# in order to show lines along the edges instead of the gradient direction rotate by 90 degree\n", "visualization.history_of_oriented_gradients(img, hog_bins.reshape(16, 8, num_bins))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["For the first example in the dataset (`idx=0,patch_idx=0`), the visualization should look something like this:\n", "\n", "![Example Visualization](./hog_visualization.png)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Train a Classifier"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["In order to get the classifier part of an object detector capable of deciding whether in a given input patch an object is visible or not, we now train a small network, wich takes the handcrafted input features and computes a discision based on processing these features.\n", "For training the binary classifier we use the binary cross entropy loss and the Adam optimizer.\n", "During training, the accuracy $\\frac{ TP+TN}{(TP+TN+FP+FN)}$ on the training set is reported for each training batch.\n", "For evaluation we use the test set and compute the accuracy in the same way.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_train = MOT16HoG(dataset_dir.joinpath(\"MOT16\", \"train\"), transforms=obj_detect_transforms(train=False))\n", "dataset_test = MOT16HoG(dataset_dir.joinpath(\"MOT16\", \"test\"), random_offset=False, transforms=obj_detect_transforms(train=False))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_epochs = 3\n", "dataloader = DataLoader(dataset_train, batch_size=8)\n", "model = Net(activation=torch.nn.ReLU(), input_size=3780, classes=1)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Store the model for submission. You can also test different architectures and revisit the trained models. Make sure that in the submission, the model.pt and the definition of the model in your python file correspond."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.to(device)\n", "loss = torch.nn.BCELoss()\n", "optimizer = optim.Adam(params=model.parameters(), lr=1.0e-4)\n", "\n", "train(dataloader, model, loss, optimizer, num_epochs, device)\n", "torch.save(model.state_dict(), output_dir.joinpath(\"model.pt\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluate(DataLoader(dataset_test, batch_size=16), model, device)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["As accuracy for unbalanced data is a limited measure for the performance, we should also focus on imporoving precision and recall.\n", "Note that, in general, the accuracy of the whole object detector is calculated differently. The model is soving for the classification task only. In contrast, an object detector process an uncropped image. A 2-stage will first get object proposals (e.g. crops of the image) and then output a refined region of an object (regression) and its class (classification)."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part IV - Sliding Window Detection\n", "\n", "An object detector is more than just a classifier and has to solve the regression and object proposal task as well. Sliding window represents early and naive approaches to object detection. Given an image patches with a fixed resolution are extracted based on a sliding window manner. These patches are then classified as belonging to an object or not. Possible classifiers include template matching but also SVMs and neural networks. In this exercise, we will be using the classifier network trained in the last part of the exercise. Your task is to extract the image patches from a given image and then classify them and store the results in a heatmap.\n", "\n", "As sliding window approaches are slow, a stride parameter - similar to CNNs - can be used to reduce the number of patches. Given an image of size $H \\times W$ and a patch size of $H_p \\times W_p$ the output heatmap has size\n", "$$\\lfloor \\frac{H-(H_p-1)-1}{stride}+1 \\rfloor, \\lfloor \\frac{W-(W_p-1)-1}{stride}+1\\rfloor\\,.$$"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "    <h3>Task: Sliding Window Detection</h3>\n", "    <p>\n", "    Go to <code> exercisecode/model/sliding_window_detection.py</code> and implement <code>sliding_window_detection</code>.\n", "    </p>\n", "</div>"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    <h3>Test: Sliding Window Detection</h3>\n", "    <p> Run the following cell to execute the test case for the Sliding Window. As always the test are done with small dummy examples and do <b>not</b> guarantee correct implementation.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_ = test_sliding_window_detection()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idx = 0\n", "load_trained_model = False\n", "if load_trained_model:\n", "    model.load_state_dict(torch.load(output_dir.joinpath(\"model.pt\")))\n", "    model.to(device)\n", "#img = dataset_train.get_whole_image(idx)\n", "img = dataset_test.get_whole_image(idx)\n", "patch_size = (128, 64)\n", "heatmap = sliding_window_detection(img, model, patch_size, stride=4) # H_, W_\n", "visualization.heatmap_objectness(img, heatmap)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part V - Non Maximum Suppression\n", "\n", "So far the object detector creates a heatmap for a given input image. This heatmap consists of classification scores for each of the patches. Typically the classifier will create high classification scores for multiple bounding boxes that overlap with an object. However, it is desirable to keep only a single bounding box per object. Non Maximum Suppression (NMS) is a widely used post-processing algorithm to only the bounding box with the highest classification score within a certain vicinity. The pseudo-code for NMS is covered in lecture 2 as well as the exercise slides accompanying this exercise."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "    <h3>Task: Non Maxmimum Suppression</h3>\n", "    <p> Go to <code>model/nms.py</code> and implement <code> non_maximum_suppression</code>. Test yourself: How does a high threshold influence the precision and the recall of the object detector? </p>\n", "</div>"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    <h3>Test: Non Maxmimum Suppression</h3>\n", "    <p> Run the following cell to execute the test case for the Non Maximum Suppression. As always the test are done with small dummy examples and do <b>not</b> guarantee correct implementation.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_ = test_non_maximum_suppression()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idx = 0\n", "block_size = 8\n", "num_bins = 9\n", "patch_size = (128, 64)\n", "pyramid_levels = [0, 1]\n", "strides = [8, 4]\n", "\n", "img = dataset_test.get_whole_image(idx)\n", "results = []\n", "for level, stride in zip(pyramid_levels, strides):\n", "    scale = 2**level\n", "    img_pyramid = transforms.functional.resize(img, [img.shape[-2] // scale, img.shape[-1] // scale])\n", "\n", "    heatmap = sliding_window_detection(img_pyramid, model, patch_size, stride) # H_, W_\n", "    result = extract_bbox_from_heatmap(heatmap, 0.5, patch_size, scale, stride )# (B,4), (B,) or None,None\n", "    results.append(result) \n", "\n", "bboxes = torch.cat([result[0] for result in results if result[0] is not None], dim=0) #B*num_levels*num_strides,4\n", "scores = torch.cat([result[1] for result in results if result[1] is not None], dim=0) #B*num_levels*num_strides\n", "print(bboxes.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["visualization.bboxes(img, bboxes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nms_bboxes = non_maximum_suppression(bboxes, scores, 0.2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["visualization.bboxes(img, nms_bboxes)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Now you are done. You can zip the entire folder using the subsequent command."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from exercise_code.submit import submit_exercise\n", "\n", "submit_exercise('../output/exercise01')"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise Part VI - Testing an SOTA object detector\n", "\n", "For comparison, we provide you with a SOTA object detector pretrained on the MOT challenge training set, which will also be used in the upcoming exercise.\n", "\n", "The object detector is a [Faster R-CNN](https://arxiv.org/abs/1506.01497) [1] with a Resnet50 feature extractor. We trained the native PyTorch implementation of Faster-RCNN. For more information check out the corresponding PyTorch [webpage](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n", "\n", "[1] [Ren et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, NeurIPS 2015](https://arxiv.org/abs/1506.01497)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "from exercise_code import (\n", "    FRCNN_FPN,\n", "    MOT16ObjDetect,\n", "    get_obj_detections,\n", "    eval_obj_detect_fixIoU\n", ")\n", "\n", "%load_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_file = output_dir.joinpath('faster_rcnn_fpn.model')\n", "nms_thresh = 0.3\n", "model_FRCNN = FRCNN_FPN(num_classes=2, nms_thresh=nms_thresh)\n", "state_dict = torch.load(model_file, map_location=lambda storage, loc: storage)\n", "model_FRCNN.load_state_dict(state_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_FRCNN.eval()\n", "model_FRCNN.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idx = 150\n", "block_size = 8\n", "num_bins = 9\n", "patch_size = (128, 64)\n", "pyramid_levels = [0, 1]\n", "strides = [4, 4]\n", "\n", "img = dataset_test.get_whole_image(idx)\n", "\n", "with torch.no_grad():\n", "    preds = model_FRCNN(img[None])[0]\n", "\n", "bboxes = preds[\"boxes\"] #B*num_levels*num_strides,4\n", "scores = preds[\"scores\"] #B*num_levels*num_strides\n", "print(bboxes.shape)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["As you noticed by running the above cell, in contrast to the previous historic approach the Faster R-CNN forward pass takes only a fraction of the time, which was needed to slide over the entire image and performing classification of all the object proposals."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["visualization.bboxes(img, bboxes)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["For testing the performance of the object detector, we load the `dataset_test_objdetect` and run the object detector for `max_num_imgs` images. The results will be used to evaluate the object detector."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_test_objdetect = MOT16ObjDetect(dataset_dir.joinpath('MOT16', 'train'),\n", "                              obj_detect_transforms(train=False))\n", "data_loader = DataLoader(dataset_test_objdetect, batch_size=1, shuffle=True, num_workers=2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = get_obj_detections(model_FRCNN, data_loader, max_num_imgs = 10)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["In order to calculate the precision and recall of the object detection we have to match predicted bounding boxes with ground truth bounding boxes. A bounding box prediction is marked as correct, if the everlap with any gound truth annotation is bigger than `IoU_th`. Also we can consider only object detections that have a higher confidence than `conf_th`. Run the evaluation below for different values of the `conf_th`. You will see that these values form the Precision-Recall-Curve. The detector will not be able to detect all objects, even with a very low confidence threshold. However, with a very high threshold, all detections will be correct."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prec, rec = eval_obj_detect_fixIoU(dataset_test_objdetect, results, IoU_th=0.6, conf_th=0.97, verbose=True)\n", "print(f\"Prec: {prec} Rec: {rec}\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["You tested the evaluation by running it for different values for `conf_th`. In order to evaluate the object detector, we have to build the Precision-Recall-Curve with varying confidence thresholds to compute the Average Precision, which is the area under the Precision-Recall-Curve (AuC).\n", "$$AP = \\int_0^1 prec(rec) d(rec)$$\n", "\n", "Furthermore, one can find a good setting for the object detection threshold by determining the highest F1-Score for different values of the confidence. The F1-Score is high, if Precision and Recall are high.\n", "$$F1 Score = \\frac{2 \\cdot prec \\cdot rec}{prec + rec}$$\n", "\n", "Think about the following and fill in the missing value (not relevant for submission):\n", "\n", "The $mAP$ also considers the dependency on the setting of the IoU threshold. Small IoU thresholds result in a (higher/lower) number of TP predictions. Therefore the recall=TP/P of the detector will be (higher/lower) for smaller IoU thresholds.\n", "\n", "![mAP Visualization](./mAP.png)\n", "\n", "The mean Average Precision will then calculate the mean of all the AUCs, for a predefined set of IoU thresholds.\n", "$$mAP = \\frac{1}{K} \\sum_k AP_{IoU=k}$$"]}], "metadata": {"kernelspec": {"display_name": "Python 3.8.10 ('cv3')", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.4"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "b38045e10271186d31b9c7cfcf32f44b81f9b46f72bad763493647421023d2a5"}}}, "nbformat": 4, "nbformat_minor": 2}